{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d9f808",
   "metadata": {},
   "source": [
    "# Preprocess Gene Annotations\n",
    "\n",
    "This notebook creates a table of gene annotations by:\n",
    "1. Querying Biomart for all Ensembl IDs in the database\n",
    "2. Querying MyGene for annotation about those IDs\n",
    "3. Querying Ensembl for the most recent Ensembl release for each ID\n",
    "4. Building a permalink to the Ensembl archive page for each ID\n",
    "\n",
    "This gene annotation table is read in by `agoradataprocessing/process.py` to be used in the `gene_info` transformation. \n",
    "\n",
    "***Note:*** *This notebook is exploratory and should eventually be converted to a Python script that is run through an automated process.*\n",
    "\n",
    "## Installation requirements\n",
    "\n",
    "#### Linux / Windows / Mac\n",
    "\n",
    "Install R: https://cran.r-project.org/\n",
    "\n",
    "Install Python and agora-data-tools following the instructions in this repository's README. This notebook assumes it is being run from the same `pipenv` virtual environment as agora-data-tools. \n",
    "\n",
    "Then install the following packages using `pip`:\n",
    "```\n",
    "pip install rpy2 mygene\n",
    "```\n",
    "\n",
    "#### Note for Macs with M1 chips (2020 and newer)\n",
    "\n",
    "Install as above, but make sure that your R installation is the arm64 version (R-4.X.X-arm64.pkg) so that the architecture matches what pip is using. \n",
    "You may also need to install an older version of `rpy2` on the Mac:\n",
    "```\n",
    "pip install rpy2==3.5.12\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc92c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rpy2.robjects import r\n",
    "import pandas as pd\n",
    "import mygene\n",
    "import numpy as np\n",
    "import requests\n",
    "import agoradatatools.etl.utils as utils\n",
    "import agoradatatools.etl.extract as extract\n",
    "import preprocessing_utils\n",
    "\n",
    "r(\n",
    "    'if (!require(\"BiocManager\", character.only = TRUE)) { install.packages(\"BiocManager\") }'\n",
    ")\n",
    "r('if (!require(\"biomaRt\")) { BiocManager::install(\"biomaRt\") }')\n",
    "\n",
    "r.library(\"biomaRt\")\n",
    "\n",
    "ensembl_ids_filename = \"../../output/ensembl_id_list.txt\"\n",
    "archive_filename = \"../../output/ensembl_archive_list.csv\"\n",
    "config_filename = \"../../../../config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36f54c",
   "metadata": {},
   "source": [
    "# Part 1: Get gene annotation data\n",
    "\n",
    "## [Deprecated] Query Biomart for a list of all Ensembl IDs in the database of human genes. \n",
    "\n",
    "Here we use the R library `biomaRt`. There is no canonical Python library with the features we need for this script. \n",
    "\n",
    "*We no longer get all genes from BioMart, so this section is unused. The code is here in case we need it again.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e115d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ensembl_ids_df = preprocessing_utils.r_query_biomart()\n",
    "ensembl_ids_df = preprocessing_utils.filter_hasgs(\n",
    "    df=ensembl_ids_df, chromosome_name_column=\"chromosome_name\"\n",
    ")\n",
    "print(str(ensembl_ids_df.shape[0]) + \" genes remaining after HASG filtering.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494e58f",
   "metadata": {},
   "source": [
    "## Get Ensembl IDs from data sets that will be processed by agora-data-tools\n",
    "\n",
    "Loop through all data sets in the config file to get all Ensembl IDs used in every data set. Exclude `gene_metadata` since that's the file we are building, and `druggability` since that data is deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdbeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_ensembl_list = preprocessing_utils.get_all_adt_ensembl_ids(\n",
    "    config_filename=config_filename,\n",
    "    exclude_files=[\"gene_metadata\", \"druggability\"],\n",
    "    token=None,\n",
    ")\n",
    "print(\"\")\n",
    "print(str(len(file_ensembl_list)) + \" Ensembl IDs found.\")\n",
    "print(file_ensembl_list[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa76bfb",
   "metadata": {},
   "source": [
    "Create a data frame with these IDs so it can be merged with the MyGene query results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1303e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_ids_df = pd.DataFrame({\"ensembl_gene_id\": file_ensembl_list})\n",
    "\n",
    "\"\"\" Removed due to no longer getting genes from BioMart, but saving code\n",
    "# Add Ensembl IDs that are in the files but not in the biomart result\n",
    "missing = set(file_ensembl_list) - set(ensembl_ids_df[\"ensembl_gene_id\"])\n",
    "print(\n",
    "    str(len(missing))\n",
    "    + \" genes from the data files are missing from Biomart results and will be added.\"\n",
    ")\n",
    "\n",
    "missing_df = pd.DataFrame({\"ensembl_gene_id\": list(missing), \"chromosome_name\": \"\"})\n",
    "ensembl_ids_df = pd.concat([ensembl_ids_df, missing_df])\n",
    "\"\"\"\n",
    "\n",
    "ensembl_ids_df = ensembl_ids_df.dropna(subset=[\"ensembl_gene_id\"])\n",
    "print(len(ensembl_ids_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a file to save the list of IDs\n",
    "ensembl_ids_df.to_csv(\n",
    "    path_or_buf=ensembl_ids_filename, sep=\"\\t\", header=False, index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ef6ec",
   "metadata": {},
   "source": [
    "## Get info on each gene from mygene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd03d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mg = mygene.MyGeneInfo()\n",
    "\n",
    "mygene_output = mg.getgenes(\n",
    "    ensembl_ids_df[\"ensembl_gene_id\"],\n",
    "    fields=[\"symbol\", \"name\", \"summary\", \"type_of_gene\", \"alias\"],\n",
    "    as_dataframe=True,\n",
    ")\n",
    "\n",
    "mygene_output.index.rename(\"ensembl_gene_id\", inplace=True)\n",
    "mygene_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb114e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Annotations found for \" + str(sum(mygene_output[\"notfound\"].isna())) + \" genes.\")\n",
    "print(\n",
    "    \"No annotations found for \"\n",
    "    + str(sum(mygene_output[\"notfound\"] == True))\n",
    "    + \" genes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf71470",
   "metadata": {},
   "source": [
    "# Part 2: Clean the data\n",
    "\n",
    "## Join and standardize columns / values\n",
    "\n",
    "For consistency with the `agora-data-tools` transform process, this uses the etl standardize functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d8cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gene_table_merged = pd.merge(\n",
    "    left=ensembl_ids_df,\n",
    "    right=mygene_output,\n",
    "    how=\"left\",\n",
    "    on=\"ensembl_gene_id\",\n",
    "    validate=\"many_to_many\",\n",
    ")\n",
    "\n",
    "gene_table_merged = utils.standardize_column_names(gene_table_merged)\n",
    "gene_table_merged = utils.standardize_values(gene_table_merged)\n",
    "\n",
    "print(gene_table_merged.shape)\n",
    "gene_table_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441115e",
   "metadata": {},
   "source": [
    "## Fix alias field\n",
    "\n",
    "Fix `NaN` values in the `alias` field and make sure every alias value is a list, not a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c10d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gene_table_merged[\"alias\"] = gene_table_merged[\"alias\"].apply(\n",
    "    preprocessing_utils.standardize_list_item\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be1b92",
   "metadata": {},
   "source": [
    "## Remove duplicate Ensembl IDs from the list. \n",
    "\n",
    "Duplicates in the list typically have the same Ensembl ID but different gene symbols. This usually happens when a single Ensembl ID maps to multiple Entrez IDs in the NCBI database. For every set of duplicated rows with the same Ensembl ID, we remove all rows but the first row in the set, and the symbols and aliases of the removed rows get added to the \"alias\" field of the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63cc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For printing only\n",
    "dupes = gene_table_merged[\"ensembl_gene_id\"].duplicated()\n",
    "dupe_ids = gene_table_merged.loc[dupes, \"ensembl_gene_id\"]\n",
    "print(\n",
    "    gene_table_merged.loc[\n",
    "        gene_table_merged[\"ensembl_gene_id\"].isin(dupe_ids),\n",
    "        [\"ensembl_gene_id\", \"symbol\", \"alias\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Remove duplicates\n",
    "gene_table_merged = preprocessing_utils.merge_duplicate_ensembl_ids(gene_table_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(dupe_ids.drop_duplicates())) + \" duplicated genes have been processed.\")\n",
    "print(gene_table_merged.shape)\n",
    "print(gene_table_merged.loc[gene_table_merged[\"ensembl_gene_id\"].isin(dupe_ids), \"alias\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c361d",
   "metadata": {},
   "source": [
    "# Part 3: Create Ensembl archive permalinks\n",
    "\n",
    "## Get a table of Ensembl archive URLs\n",
    "\n",
    "This is where we need to use the R biomaRt library specifically, instead of any of the available Python interfaces to Biomart, to get a table of Ensembl release versions and their corresponding archive URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bbdee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df = r.listEnsemblArchives()\n",
    "archive_df.to_csvfile(path=archive_filename, row_names=False, quote=False)\n",
    "\n",
    "print(archive_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e150d8e",
   "metadata": {},
   "source": [
    "## Query Ensembl for each gene's version\n",
    "\n",
    "Ensembl's REST API can only take 1000 genes at once, so this is looped to query groups of 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a747309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "versions = preprocessing_utils.query_ensembl_version_api(\n",
    "    ensembl_ids=gene_table_merged[\"ensembl_gene_id\"].tolist()\n",
    ")\n",
    "\n",
    "versions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c108238",
   "metadata": {},
   "outputs": [],
   "source": [
    "versions.groupby(\"release\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5aecb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that all IDs are the same between the result and the gene table\n",
    "print(len(versions[\"id\"]))\n",
    "print(len(gene_table_merged))\n",
    "print(\n",
    "    all(versions[\"id\"].isin(gene_table_merged[\"ensembl_gene_id\"]))\n",
    "    and all(gene_table_merged[\"ensembl_gene_id\"].isin(versions[\"id\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything is GRCh38, not GRCh37\n",
    "all(versions[\"assembly\"] == \"GRCh38\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e29252",
   "metadata": {},
   "source": [
    "## Create permalinks based on archive version\n",
    "\n",
    "**Not all of these versions have an archive.** We can go back to the closest previous archive for these but the link isn't guaranteed to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b5652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_table = pd.read_csv(archive_filename)\n",
    "\n",
    "# Remove GRCh37 from the archive list\n",
    "archive_table = archive_table[archive_table[\"version\"] != \"GRCh37\"].reset_index()\n",
    "\n",
    "archive_table[\"numeric_version\"] = archive_table[\"version\"].astype(int)\n",
    "\n",
    "\n",
    "def closest_release(release, archive_table):\n",
    "    if release in archive_table:\n",
    "        return release\n",
    "\n",
    "    return max([V for V in archive_table[\"numeric_version\"] if V <= release])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "versions[\"closest_release\"] = 0\n",
    "\n",
    "releases = versions[\"release\"].drop_duplicates().astype(int)\n",
    "\n",
    "# Only have to call closest_release once per version, instead of >70k times\n",
    "for release in releases:\n",
    "    versions.loc[versions[\"release\"] == str(release), \"closest_release\"] = (\n",
    "        closest_release(release, archive_table)\n",
    "    )\n",
    "\n",
    "versions.groupby(\"closest_release\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e5006",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "versions[\"permalink\"] = \"\"\n",
    "\n",
    "for i in versions.index:\n",
    "    match = archive_table[\"numeric_version\"] == versions.at[i, \"closest_release\"]\n",
    "    url = archive_table.loc[match, \"url\"].to_string(index=False)\n",
    "    if len(url) > 0:\n",
    "        versions.at[i, \"permalink\"] = (\n",
    "            url + \"/Homo_sapiens/Gene/Summary?db=core;g=\" + versions.at[i, \"id\"]\n",
    "        )\n",
    "\n",
    "versions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b01719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "versions[versions[\"closest_release\"] < 100].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4128cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(versions[\"permalink\"][0])\n",
    "print(versions[\"permalink\"][25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73791e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does every gene have an associated URL?\n",
    "url_base_len = len(archive_table[\"url\"][0]) + 1\n",
    "all([len(url) > url_base_len for url in versions[\"permalink\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98b62a",
   "metadata": {},
   "source": [
    "# Part 4: Add permalinks to the gene table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3edfd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = versions[[\"id\", \"release\", \"possible_replacement\", \"permalink\"]]\n",
    "versions.rename(\n",
    "    columns={\"id\": \"ensembl_gene_id\", \"release\": \"ensembl_release\"}, inplace=True\n",
    ")\n",
    "\n",
    "gene_table_merged = pd.merge(\n",
    "    left=gene_table_merged,\n",
    "    right=versions,\n",
    "    how=\"left\",\n",
    "    on=\"ensembl_gene_id\",\n",
    "    validate=\"one_to_one\",\n",
    ")\n",
    "\n",
    "print(gene_table_merged.shape)\n",
    "gene_table_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eba7c2",
   "metadata": {},
   "source": [
    "### Final cleanup\n",
    "\"possible_replacement\" entries will either be an empty list or a list of dictionaries. Entries that have data in them need to have the Ensembl IDs pulled out of them as a list of strings.\n",
    "\n",
    "Remove unneeded columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c07b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_table_merged[\"possible_replacement\"] = gene_table_merged[\n",
    "    \"possible_replacement\"\n",
    "].apply(lambda pr: pr if len(pr) == 0 else [x[\"stable_id\"] for x in pr])\n",
    "\n",
    "gene_table_merged = gene_table_merged[\n",
    "    [\n",
    "        \"ensembl_gene_id\",\n",
    "        \"name\",\n",
    "        \"alias\",\n",
    "        \"summary\",\n",
    "        \"symbol\",\n",
    "        \"type_of_gene\",\n",
    "        \"ensembl_release\",\n",
    "        \"possible_replacement\",\n",
    "        \"permalink\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "gene_table_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b371435",
   "metadata": {},
   "source": [
    "### Write to a file\n",
    "This will get uploaded to Synapse as [syn25953363](https://www.synapse.org/#!Synapse:syn25953363)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2287922",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_table_merged = gene_table_merged.sort_values(by=\"ensembl_gene_id\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "gene_table_merged\n",
    "gene_table_merged.to_feather(\"../../output/gene_table_merged_GRCh38.p14.feather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agora-data-tools-ywFp1Gf9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
